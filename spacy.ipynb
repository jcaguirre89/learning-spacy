{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to SpaCy\n",
    "Following allong [this course](https://course.spacy.io/chapter1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "TEXT = \"Hello World! My Name is Cristobal, I live in Canada and am 29 years old\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.prefer_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = English()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(TEXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index:    [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n",
      "Text:     ['Hello', 'World', '!', 'My', 'Name', 'is', 'Cristobal', ',', 'I', 'live', 'in', 'Canada', 'and', 'am', '29', 'years', 'old']\n",
      "is_alpha: [True, True, False, True, True, True, True, False, True, True, True, True, True, True, False, True, True]\n",
      "is_punct: [False, False, True, False, False, False, False, True, False, False, False, False, False, False, False, False, False]\n",
      "like_num: [False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False]\n"
     ]
    }
   ],
   "source": [
    "# Doc is a collection of tokens initialized with `nlp()`--each token is an object with attributes that describe it.\n",
    "\n",
    "# Token attributes\n",
    "print('Index:   ', [token.i for token in doc])\n",
    "print('Text:    ', [token.text for token in doc])\n",
    "\n",
    "print('is_alpha:', [token.is_alpha for token in doc])\n",
    "print('is_punct:', [token.is_punct for token in doc])\n",
    "print('like_num:', [token.like_num for token in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Models\n",
    "## What are statistical models?\n",
    "- Enable spaCy to predict linguistic attributes in context\n",
    "    - Part-of-speech tags\n",
    "    - Syntactic dependencies\n",
    "    - Named entities\n",
    "- Trained on labeled example texts\n",
    "- Can be updated with more examples to fine-tune predictions\n",
    "\n",
    "## Installation\n",
    "Download a trained model with `python -m spacy download en_core_web_sm`, which then allows you to\n",
    "run `nlp = spacy.load('en_core_web_sm')` to load it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(TEXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('New', 'PROPN'),\n",
      " ('Iphone', 'PROPN'),\n",
      " ('X', 'PROPN'),\n",
      " ('resease', 'NOUN'),\n",
      " ('date', 'NOUN'),\n",
      " ('leaked', 'VERB'),\n",
      " ('.', 'PUNCT'),\n",
      " ('I', 'PRON'),\n",
      " ('am', 'VERB'),\n",
      " ('going', 'VERB'),\n",
      " ('to', 'PART'),\n",
      " ('buy', 'VERB'),\n",
      " ('an', 'DET'),\n",
      " ('iphone', 'NOUN'),\n",
      " ('then', 'ADV'),\n",
      " ('.', 'PUNCT')]\n"
     ]
    }
   ],
   "source": [
    "# Predicting Part-Of-Speech tags\n",
    "pprint.pprint([(token.text, token.pos_) for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('New', 'compound', 'X'),\n",
      " ('Iphone', 'compound', 'X'),\n",
      " ('X', 'compound', 'date'),\n",
      " ('resease', 'compound', 'date'),\n",
      " ('date', 'nsubj', 'leaked'),\n",
      " ('leaked', 'ROOT', 'leaked'),\n",
      " ('.', 'punct', 'leaked'),\n",
      " ('I', 'nsubj', 'going'),\n",
      " ('am', 'aux', 'going'),\n",
      " ('going', 'ROOT', 'going'),\n",
      " ('to', 'aux', 'buy'),\n",
      " ('buy', 'xcomp', 'going'),\n",
      " ('an', 'det', 'iphone'),\n",
      " ('iphone', 'dobj', 'buy'),\n",
      " ('then', 'advmod', 'buy'),\n",
      " ('.', 'punct', 'going')]\n"
     ]
    }
   ],
   "source": [
    "# Predicting Dependency\n",
    "# In addition to the part-of-speech tags, we can also predict how the words are related. For example, whether a word is the subject of the sentence or an object.\n",
    "# The head attribute returns the syntactic head token. You can also think of it as the parent token this word is attached to.\n",
    "\n",
    "pprint.pprint([(token.text, token.dep_, token.head.text) for token in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt](spacy-dependency-scheme.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entities:    [('Cristobal', 'GPE'), ('Canada', 'GPE'), ('29 years old', 'DATE')]\n",
      "Countries, cities, states\n"
     ]
    }
   ],
   "source": [
    "# Predicting named entities\n",
    "# The ents attribute in doc allows access to the predicted named entities.\n",
    "# It returns an iterator of Span objects, so we can print the entity text and the entity label using the label_ attribute.\n",
    "print('Entities:   ', [(span.text, span.label_) for span in doc.ents])\n",
    "\n",
    "# Can also get descriptions of the entity labels\n",
    "print(spacy.explain('GPE'))\n",
    "\n",
    "# Note that it get's my name wrong, probably because it's spanish and this is trained only on english text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rule Based Matching\n",
    "Compared to regular expressions, the matcher works with Doc and Token objects instead of only strings.\n",
    "\n",
    "It's also more flexible: you can search for texts but also other lexical attributes.\n",
    "\n",
    "You can even write rules that use the model's predictions.\n",
    "\n",
    "For example, find the word \"duck\" only if it's a verb, not a noun.\n",
    "\n",
    "Match patterns are lists of dictionaries. Each dictionary describes one token. The keys are the names of token attributes, mapped to their expected values.\n",
    "\n",
    "The matcher dot add method lets you add a pattern. The first argument is a unique ID to identify which pattern was matched. The second argument is an optional callback. We don't need one here, so we set it to None. The third argument is the pattern.\n",
    "\n",
    "To match the pattern on a text, we can call the matcher on any doc.\n",
    "\n",
    "This will return the matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "In this example, we're looking for two tokens with the text \"iPhone\" and \"X\".\n",
    "\n",
    "We can also match on other token attributes. Here, we're looking for two tokens whose lowercase forms equal \"iphone\" and \"x\".\n",
    "\n",
    "We can even write patterns using attributes predicted by the model. Here, we're matching a token with the lemma \"buy\", plus a noun. The lemma is the base form, so this pattern would match phrases like \"buying milk\" or \"bought flowers\".\n",
    "\n",
    "The matcher is initialized with the shared vocabulary, nlp dot vocab. You'll learn more about this later â€“ for now, just remember to always pass it in.\n",
    "\n",
    "\"OP\" can have one of four values:\n",
    "{'OP': '!'}\tNegation: match 0 times\n",
    "{'OP': '?'}\tOptional: match 0 or 1 times\n",
    "{'OP': '+'}\tMatch 1 or more times\n",
    "{'OP': '*'}\tMatch 0 or more times\n",
    "\"\"\"\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "matcher = Matcher(nlp.vocab)\n",
    "doc = nlp('New Iphone X resease date leaked. I am going to buy an iphone then.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iphone X\n",
      "buy an iphone\n"
     ]
    }
   ],
   "source": [
    "# Match exact token texts\n",
    "pattern = [{'TEXT': 'iPhone'}, {'TEXT': 'X'}]\n",
    "\n",
    "# Match lexical attributes\n",
    "# Won't match because \n",
    "pattern_1 = [{'TEXT': 'Iphone'}, {'UPPER': 'X'}]\n",
    "\n",
    "# Match any token attributes\n",
    "pattern_2 = [{'LEMMA': 'buy'}, {'POS': 'DET', 'OP': '?'}, {'POS': 'NOUN'}]\n",
    "          \n",
    "matcher.add('IPHONE_PATTERN', None, pattern_1)\n",
    "matcher.add('BUYING_PATTERN', None, pattern_2)\n",
    "matches = matcher(doc)\n",
    "for match_id, start, end in matches:\n",
    "    matched_span = doc[start:end]\n",
    "    print(matched_span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Structures\n",
    "## Vocab\n",
    "spaCy stores all shared data in a vocabulary, the Vocab.\n",
    "\n",
    "This includes words, but also the labels schemes for tags and entities.\n",
    "\n",
    "To save memory, all strings are encoded to hash IDs. If a word occurs more than once, we don't need to save it every time.\n",
    "\n",
    "Instead, spaCy uses a hash function to generate an ID and stores the string only once in the string store. The string store is available as `nlp.vocab.strings`.\n",
    "\n",
    "It's a lookup table that works in both directions. You can look up a string and get its hash, and look up a hash to get its string value. Internally, spaCy only communicates in hash IDs.\n",
    "\n",
    "Hash IDs can't be reversed, though. If a word in not in the vocabulary, there's no way to get its string. That's why we always need to pass around the shared vocab.\n",
    "\n",
    "## Lexemes\n",
    "Lexemes are context-independent entries in the vocabulary.\n",
    "\n",
    "You can get a lexeme by looking up a string or a hash ID in the vocab.\n",
    "\n",
    "Lexemes expose attributes, just like tokens.\n",
    "\n",
    "They hold context-independent information about a word, like the text, or whether the the word consists of alphanumeric characters.\n",
    "\n",
    "Lexemes don't have part-of-speech tags, dependencies or entity labels. Those depend on the context. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coffee 3197928453018144401 True\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"I love coffee\")\n",
    "lexeme = nlp.vocab['coffee']\n",
    "\n",
    "# Print the lexical attributes\n",
    "print(lexeme.text, lexeme.orth, lexeme.is_alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word vectors and semantic similarity\n",
    "spaCy can compare two objects and predict how similar they are â€“ for example, documents, spans or single tokens.\n",
    "\n",
    "The Doc, Token and Span objects have a dot similarity method that takes another object and returns a floating point number between 0 and 1, indicating how similar they are.\n",
    "\n",
    "One thing that's very important: In order to use similarity, you need a larger spaCy model that has word vectors included.\n",
    "\n",
    "For example, the medium or large English model â€“ but not the small one. So if you want to use vectors, always go with a model that ends in \"md\" or \"lg\". You can find more details on this in the models documentation.\n",
    "\n",
    "## Word Vectors\n",
    "But how does spaCy do this under the hood?\n",
    "\n",
    "Similarity is determined using word vectors, multi-dimensional representations of meanings of words.\n",
    "\n",
    "You might have heard of Word2Vec, which is an algorithm that's often used to train word vectors from raw text.\n",
    "\n",
    "Vectors can be added to spaCy's statistical models.\n",
    "\n",
    "By default, the similarity returned by spaCy is the cosine similarity between two vectors â€“ but this can be adjusted if necessary.\n",
    "\n",
    "Vectors for objects consisting of several tokens, like the Doc and Span, default to the average of their token vectors.\n",
    "\n",
    "That's also why you usually get more value out of shorter phrases with fewer irrelevant words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-5.1171e-01 -1.0681e-01 -4.0689e-01 -3.2231e-01  1.8074e-01  3.7749e-01\n",
      "  4.3708e-02 -2.8818e-01  2.7681e-01  1.0651e+00 -3.8360e-01  2.9665e-01\n",
      " -4.8123e-01  8.7665e-02 -1.1448e-01 -6.6952e-01 -3.7934e-02  1.9361e+00\n",
      " -5.5409e-01  2.3255e-01  5.6811e-01  1.2107e-01 -2.9582e-01  2.0801e-01\n",
      "  3.5881e-01 -1.6022e-01 -3.7637e-01 -2.0065e-01 -1.4099e-01  5.8564e-01\n",
      " -5.5319e-01  8.7606e-02  4.0345e-01  3.2074e-01  4.8916e-01 -5.8248e-01\n",
      "  2.5284e-01  4.5514e-01 -4.7540e-01 -2.2623e-01  1.9731e-01 -5.9291e-01\n",
      "  8.0345e-02 -4.6022e-02 -1.7881e-01 -1.1827e-01 -5.1545e-03  9.0144e-02\n",
      " -3.2856e-02  2.3501e-01  1.9211e-01  2.4003e-01 -2.9716e-01  4.4344e-02\n",
      "  1.0656e-01 -1.6923e-01  3.4344e-01  1.8520e-01 -2.6764e-01 -1.3614e-01\n",
      "  1.0132e-01  1.5200e-01  1.5203e-01  2.6930e-01 -3.1428e-01  1.1998e-01\n",
      " -4.0277e-02  3.0855e-01  2.7413e-01  1.4967e-01  1.3133e-01  5.9081e-01\n",
      " -4.1874e-01 -5.3135e-01  3.2925e-01 -2.1868e-01 -8.1421e-01  3.5644e-01\n",
      "  4.3644e-01 -7.0250e-01  5.7554e-01  3.8241e-01 -1.6218e-01  3.8100e-01\n",
      " -1.3327e-01 -1.0647e-01  1.3157e+00 -9.4133e-01 -7.9785e-02  3.5727e-01\n",
      " -1.8197e-02  2.2250e-03 -2.9857e-01 -3.3645e-01  4.4106e-01  1.5041e-01\n",
      "  1.6043e-01  2.6560e-02 -4.0346e-02  1.2727e-01  4.7279e-01  4.1032e-01\n",
      "  1.6767e-01 -2.6203e-01 -2.3548e-01  2.1169e-01  3.6358e-01  5.6634e-02\n",
      " -3.5832e-02  5.6448e-04 -1.4364e-01 -2.8075e-02 -4.0712e-02  4.1439e-02\n",
      " -8.3740e-02 -1.3098e-01 -1.2311e-01 -1.4595e-01  2.8795e-02  7.9492e-02\n",
      " -1.1515e-01  2.7680e-01 -2.7946e-01 -2.1907e-01 -5.0950e-01  5.8333e-01\n",
      "  9.5262e-02 -1.4653e-01 -9.9257e-02 -8.3423e-02  3.1052e-01  3.8155e-01\n",
      "  1.7791e-01 -2.3998e-02  2.4078e-01  3.7037e-01  5.1498e-01 -3.4893e-01\n",
      " -3.1809e-01 -2.4122e-01 -5.9769e-01  6.4805e-02  3.1365e-01 -1.0607e-02\n",
      "  1.0595e-01 -1.5077e-01  3.1043e-01  1.1138e-01 -8.1244e-02 -5.1163e-01\n",
      "  9.7784e-02  3.0142e-01  1.3174e-01 -1.1455e-01  1.8674e-01 -9.8812e-02\n",
      " -2.5187e-01 -3.2030e-01 -4.4983e-02 -4.6903e-02  4.9917e-01 -3.7893e-01\n",
      "  2.6565e-01 -3.9412e-01 -2.9966e-02 -2.7698e-01  2.6036e-01  2.4774e-01\n",
      " -9.3104e-02  2.7786e-01 -5.2087e-02 -2.8428e-01 -2.6613e-02 -3.0993e-01\n",
      "  4.4573e-01 -4.6314e-01 -2.2311e-02 -2.7138e-02 -7.1779e-02  1.9421e-01\n",
      "  5.9056e-01 -2.3239e-01  2.5022e-01 -1.0690e-01 -2.2288e-01  4.9371e-01\n",
      " -3.1091e-01  3.3129e-01  3.3988e-01  1.6750e-01  1.5585e-01  5.5716e-01\n",
      " -1.7261e-01 -4.6732e-01  3.8138e-01 -2.6424e-01 -1.8118e-01 -8.5095e-01\n",
      " -1.6320e-01 -1.6973e-01  3.2208e-01 -1.4497e-01  1.8021e-01  1.5971e-01\n",
      "  3.4631e-01 -2.7139e-01 -2.0807e-01  1.2101e-01 -5.5328e-01 -3.1268e-01\n",
      "  1.6741e-02  5.0859e-01  6.9877e-02  4.0493e-02 -3.4586e-01  2.5514e-01\n",
      " -5.9900e-01  3.2319e-01 -3.3612e-01  4.1495e-01  3.8633e-01 -2.3185e-02\n",
      "  1.0689e-01  1.5322e-02 -1.1313e-01  1.2154e-01 -5.4214e-01 -3.4194e-01\n",
      "  2.3739e-01  1.3031e-01 -1.7151e-01 -5.0186e-01  2.5582e-01  2.0040e-01\n",
      "  1.2656e-01 -9.9249e-02 -4.8007e-02  1.5751e-01 -7.3054e-01  2.4785e-01\n",
      "  2.0304e-01  1.5305e-01  3.3658e-01  5.9503e-01 -1.4675e-01  6.1928e-01\n",
      "  5.4733e-01 -1.0057e-01 -6.4301e-01 -4.1983e-01  7.1912e-01  1.3608e-01\n",
      "  2.7029e-01 -1.7334e-01 -4.2572e-01 -4.2156e-01  2.3917e-01 -2.9549e-01\n",
      " -1.5772e-03  4.3726e-01 -3.8201e-01  1.7429e-01  2.9893e-01  1.3506e-01\n",
      " -2.3199e-01 -3.6210e-01  3.0076e-01 -1.8244e-01  8.1885e-01 -6.7392e-01\n",
      "  1.6363e-01  2.0264e-01 -1.3863e-01  2.6357e-01  2.6444e-01 -2.9205e-01\n",
      " -3.0595e-01  1.6506e-01 -6.1202e-01  1.5182e-01 -2.4218e-03  6.7364e-02\n",
      " -1.3306e-01  1.4539e-01 -5.5739e-02 -1.8543e-01  3.3690e-02  1.8207e-01\n",
      "  2.7962e-01 -2.4353e-01  1.6472e-01 -5.8872e-01 -6.5259e-01 -1.1114e-01\n",
      "  5.4754e-01  2.4392e-02 -2.0944e-01 -1.9557e-01  9.4377e-02  1.4286e-01]\n"
     ]
    }
   ],
   "source": [
    "# Word vectors\n",
    "doc = list(nlp.pipe('banana'))\n",
    "token = doc[0]\n",
    "print(token.vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', True, 6.4231944, False), ('love', True, 6.04035, False), ('pizza', True, 7.0450306, False)]\n",
      "0.810140967454893\n",
      "0.657904\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_md')\n",
    "doc1 = nlp('I like fast food')\n",
    "doc2 = nlp('I love pizza')\n",
    "\n",
    "# Doc-level similarity\n",
    "print(doc1.similarity(doc2))\n",
    "# Token-level similarity\n",
    "print(doc1[1].similarity(doc2[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9501447503553421\n"
     ]
    }
   ],
   "source": [
    "# Similarity and context\n",
    "# the following phrases have high similarity. It makes sense because both express a feeling about cats,\n",
    "# but if looking at sentiment, they should be considered very dissimilar\n",
    "doc1 = nlp(\"I like cats\")\n",
    "doc2 = nlp(\"I hate cats\")\n",
    "\n",
    "print(doc1.similarity(doc2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining models and rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched span: Golden Retriever\n"
     ]
    }
   ],
   "source": [
    "# PhraseMatcher\n",
    "# Like Matcher, but takes a Doc object as the pattern\n",
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "\n",
    "pattern = nlp(\"Golden Retriever\")\n",
    "matcher.add('DOG', None, pattern)\n",
    "doc = nlp(\"I have a Golden Retriever\")\n",
    "\n",
    "# Iterate over the matches\n",
    "for match_id, start, end in matcher(doc):\n",
    "    # Get the matched span\n",
    "    span = doc[start:end]\n",
    "    print('Matched span:', span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing Pipelines\n",
    "![alt text](spacy-nlp-pipeline.PNG)\n",
    "\n",
    "First, the tokenizer is applied to turn the string of text into a Doc object. Next, a series of pipeline components is applied to the Doc in order. In this case, the tagger, then the parser, then the entity recognizer. Finally, the processed Doc is returned, so you can work with it.\n",
    "\n",
    "spaCy ships with the following built-in pipeline components.\n",
    "\n",
    "- The part-of-speech tagger sets the token dot tag attribute.\n",
    "- The depdendency parser adds the token dot dep and token dot head attributes and is also responsible for detecting sentences and base noun phrases, also known as noun chunks.\n",
    "- The named entity recognizer adds the detected entities to the doc dot ents property. It also sets entity type attributes on the tokens that indicate if a token is part of an entity or not.\n",
    "- Finally, the text classifier sets category labels that apply to the whole text, and adds them to the doc dot cats property. Because text categories are always very specific, the text classifier is not included in any of the pre-trained models by default. But you can use it to train your own system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tagger', 'parser', 'ner']\n",
      "[('tagger', <spacy.pipeline.pipes.Tagger object at 0x7ff7a65967f0>), ('parser', <spacy.pipeline.pipes.DependencyParser object at 0x7ff7d2861588>), ('ner', <spacy.pipeline.pipes.EntityRecognizer object at 0x7ff7d28615e8>)]\n"
     ]
    }
   ],
   "source": [
    "print(nlp.pipe_names)\n",
    "print(nlp.pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Components\n",
    "## Anatomy of a component\n",
    "Function that takes a doc, modifies it and returns it\n",
    "Can be added using the `nlp.add_pipe` method\n",
    "```python\n",
    "def custom_component(doc):\n",
    "    # Do something to the doc here\n",
    "    return doc\n",
    "\n",
    "nlp.add_pipe(custom_component)\n",
    "```\n",
    "\n",
    "To specify where to add the component in the pipeline, you can use the following keyword arguments:\n",
    "\n",
    "|Argument | Description | Example|\n",
    "|---------|--------------|------|\n",
    "|last | If True, add last | `nlp.add_pipe(component, last=True`)|\n",
    "|first | If True, add first | `nlp.add_pipe(component, first=True`)|\n",
    "|before | Add before component | `nlp.add_pipe(component, before='ner'`)|\n",
    "|after | Add after component | `nlp.add_pipe(component, after='tagger'`)|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom component that uses PhraseMatcher to find animal names in Doc and add matched spans to the doc.ents\n",
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from spacy.tokens import Span\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'spacy.tokens.doc.Doc'>\n"
     ]
    }
   ],
   "source": [
    "animals = ['Golden Retriever', 'cat', 'turtle', 'bat']\n",
    "# nlp.pipe method returns a generator of Doc objects, so animal_patterns here below is just a list of Doc objects.\n",
    "# So it's just a much faster way of doing: docs = [nlp(text) for text in LOTS_OF_TEXTS]\n",
    "animal_patterns = list(nlp.pipe(animals))\n",
    "print(type(animal_patterns[0]))\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "matcher.add('ANIMAL', None, *animal_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tagger', 'parser', 'ner', 'animal_component']\n"
     ]
    }
   ],
   "source": [
    "def animal_component(doc):\n",
    "    # Apply matches to the doc\n",
    "    matches = matcher(doc)\n",
    "    spans = [Span(doc, start, end, label='ANIMAL') for _, start, end in matches]\n",
    "    # Overwrite the doc.ents with the matched spans\n",
    "    doc.ents = spans\n",
    "    return doc\n",
    "\n",
    "# Add component after the 'ner' component\n",
    "nlp.add_pipe(animal_component, after='ner')\n",
    "print(nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('cat', 'ANIMAL'), ('Golden Retriever', 'ANIMAL'), ('bat', 'ANIMAL')]\n"
     ]
    }
   ],
   "source": [
    "#Try it out\n",
    "# doc = nlp(\"I have a cat, a Golden Retriever\")\n",
    "doc = nlp(\"I have a cat, a Golden Retriever and a bat\")\n",
    "print([(span.text, span.label_) for span in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom attributes and methods\n",
    "Create and access custom properties on Doc, Token and Span objects via the _ attribute: \n",
    "```python\n",
    "doc._.title = 'My Title'\n",
    "token._.is_color = True\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blue - True\n"
     ]
    }
   ],
   "source": [
    "# Registered on the global Doc, Token or Span using the set_extension method\n",
    "\n",
    "from spacy.tokens import Token\n",
    "\n",
    "# Define getter function\n",
    "\"\"\"\n",
    "getter function must take one argument (the object to extend). Called only when the object\n",
    "is retrieved. Whatever is returned, is what get's shown when the particular custom attribute is called\n",
    "\"\"\"\n",
    "def get_is_color(token):\n",
    "    colors = ['red', 'yellow', 'blue']\n",
    "    return token.text in colors\n",
    "\n",
    "# Set extension on the Token with getter (force=True so that it overwrites if existing)\n",
    "Token.set_extension('is_color', getter=get_is_color, force=True)\n",
    "\n",
    "#doc = nlp(\"The sky is green.\")\n",
    "doc = nlp(\"The sky is blue.\")\n",
    "print(doc[3].text, '-', doc[3]._.is_color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True - sky is blue\n",
      "False - The sky\n"
     ]
    }
   ],
   "source": [
    "from spacy.tokens import Span\n",
    "\n",
    "def get_has_color(span):\n",
    "    colors = ['red', 'yellow', 'blue']\n",
    "    return any(token.text in colors for token in span)\n",
    "\n",
    "Span.set_extension('has_color', getter=get_has_color)\n",
    "\n",
    "doc = nlp(\"The sky is blue.\")\n",
    "print(doc[1:4]._.has_color, '-', doc[1:4].text)\n",
    "print(doc[0:2]._.has_color, '-', doc[0:2].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True - blue\n",
      "False - cloud\n"
     ]
    }
   ],
   "source": [
    "# Method extensions\n",
    "# Instead of passing a `getter` arg to `set_extension`, pass a callable in the `method` arg\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "def has_token(doc, token_text):\n",
    "    \"\"\" First argument is always de object itself, so it's like `self` \"\"\"\n",
    "    return token_text in [token.text for token in doc]\n",
    "\n",
    "Doc.set_extension('has_token', method=has_token)\n",
    "\n",
    "doc = nlp(\"The sky is blue.\")\n",
    "print(doc._.has_token('blue'), '- blue')\n",
    "print(doc._.has_token('cloud'), '- cloud')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance improvement tips\n",
    "\n",
    "## Context\n",
    "`nlp.pipe` supports passing in tuples of (text, context) with the `as_tuples` kwarg, where context is additional metadata on the text (a dictionary). \n",
    "Then instead of returning a list of `Doc` objects, it returns a list of tuples `(Doc, context)`, where context is the given dictionary\n",
    "```python\n",
    "data = [\n",
    "    ('This is a text', {'id': 1, 'page_number': 15}),\n",
    "    ('And another text', {'id': 2, 'page_number': 16}),\n",
    "]\n",
    "\n",
    "for doc, context in nlp.pipe(data, as_tuples=True):\n",
    "    print(doc.text, context['page_number'])\n",
    "```\n",
    "\n",
    "##  Using only the tokenizer\n",
    "Use nlp.make_doc to turn a text in to a Doc object\n",
    "\n",
    "BAD:\n",
    "```python\n",
    "doc = nlp(\"Hello world\")\n",
    "```\n",
    "\n",
    "GOOD:\n",
    "```python\n",
    "doc = nlp.make_doc(\"Hello world!\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and updating models\n",
    "## How training works (for a new model)\n",
    "1. Initialize the model weights randomly with `nlp.begin_training`\n",
    "2. Predict a few examples with the current weights by calling `nlp.update`\n",
    "3. Compare prediction with true labels\n",
    "4. Calculate how to change weights to improve predictions\n",
    "5. Update weights slightly\n",
    "6. Go back to 2.\n",
    "\n",
    "## Creating training data\n",
    "spaCy expects as training data a list of entries---Each entry in `TRAIN_DATA` should be a tuple with the text as the first item, and a dictionary with the annotations. Example annotations:\n",
    "```python\n",
    "{\n",
    "   \"entities\": [(0, 4, \"ORG\")],\n",
    "   \"heads\": [1, 1, 1, 5, 5, 2, 7, 5],\n",
    "   \"deps\": [\"nsubj\", \"ROOT\", \"prt\", \"quantmod\", \"compound\", \"pobj\", \"det\", \"npadvmod\"],\n",
    "   \"tags\": [\"PROPN\", \"VERB\", \"ADP\", \"SYM\", \"NUM\", \"NUM\", \"DET\", \"NOUN\"],\n",
    "   \"cats\": {\"BUSINESS\": 1.0},\n",
    "}\n",
    "```\n",
    "\n",
    "### `entities`\n",
    "Train/update the `ner` pipeline, to teach the model how to recognize new entities. values in the dictionary must be lists of tuples, each with\n",
    "3 items: `start_position`, `end_position`, `entity_label`. Sample train data:\n",
    "```python\n",
    "# Note: If you're using an existing model, make sure to mix in examples of\n",
    "# other entity types that spaCy correctly recognized before. Otherwise, your\n",
    "# model might learn the new type, but \"forget\" what it previously knew.\n",
    "# https://explosion.ai/blog/pseudo-rehearsal-catastrophic-forgetting\n",
    "TRAIN_DATA = [\n",
    "    (\n",
    "        \"Horses are too tall and they pretend to care about your feelings\",\n",
    "        {\"entities\": [(0, 6, LABEL)]},\n",
    "    ),\n",
    "    (\"Do they bite?\", {\"entities\": []}),\n",
    "    (\n",
    "        \"horses are too tall and they pretend to care about your feelings\",\n",
    "        {\"entities\": [(0, 6, LABEL)]},\n",
    "    ),\n",
    "    (\"horses pretend to care about your feelings\", {\"entities\": [(0, 6, LABEL)]}),\n",
    "    (\n",
    "        \"they pretend to care about your feelings, those horses\",\n",
    "        {\"entities\": [(48, 54, LABEL)]},\n",
    "    ),\n",
    "    (\"horses?\", {\"entities\": [(0, 6, LABEL)]}),\n",
    "]\n",
    "```\n",
    "### `heads` & `deps`\n",
    "Used to train the dependency parser. NOT SURE HOW THIS WORKS YET. Sample train data:\n",
    "```python\n",
    "TRAIN_DATA = [\n",
    "    (\n",
    "        \"They trade mortgage-backed securities.\",\n",
    "        {\n",
    "            \"heads\": [1, 1, 4, 4, 5, 1, 1],\n",
    "            \"deps\": [\"nsubj\", \"ROOT\", \"compound\", \"punct\", \"nmod\", \"dobj\", \"punct\"],\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        \"I like London and Berlin.\",\n",
    "        {\n",
    "            \"heads\": [1, 1, 1, 2, 2, 1],\n",
    "            \"deps\": [\"nsubj\", \"ROOT\", \"dobj\", \"cc\", \"conj\", \"punct\"],\n",
    "        },\n",
    "    ),\n",
    "]\n",
    "```\n",
    "### `tags`\n",
    "Train the Part-of-speech tagger. Sample train data:\n",
    "```python\n",
    "# You need to define a mapping from your data's part-of-speech tag names to the\n",
    "# Universal Part-of-Speech tag set, as spaCy includes an enum of these tags.\n",
    "# See here for the Universal Tag Set:\n",
    "# http://universaldependencies.github.io/docs/u/pos/index.html\n",
    "# You may also specify morphological features for your tags, from the universal\n",
    "# scheme.\n",
    "TAG_MAP = {\"N\": {\"pos\": \"NOUN\"}, \"V\": {\"pos\": \"VERB\"}, \"J\": {\"pos\": \"ADJ\"}}\n",
    "\n",
    "# Usually you'll read this in, of course. Data formats vary. Ensure your\n",
    "# strings are unicode and that the number of tags assigned matches spaCy's\n",
    "# tokenization. If not, you can always add a 'words' key to the annotations\n",
    "# that specifies the gold-standard tokenization, e.g.:\n",
    "# (\"Eatblueham\", {'words': ['Eat', 'blue', 'ham'], 'tags': ['V', 'J', 'N']})\n",
    "TRAIN_DATA = [\n",
    "    (\"I like green eggs\", {\"tags\": [\"N\", \"V\", \"J\", \"N\"]}),\n",
    "    (\"Eat blue ham\", {\"tags\": [\"V\", \"J\", \"N\"]}),\n",
    "]\n",
    "```\n",
    "\n",
    "### `cats`\n",
    "Used when training a text classification model. data must be lists of tuples and corresponding class:\n",
    "```python\n",
    "TRAIN_DATA = [\n",
    "    (\"I like green eggs\", 'POSITIVE'),\n",
    "    (\"I hate breakfast\", 'NEGATIVE'),\n",
    "]\n",
    "```\n",
    "\n",
    "### Training\n",
    "\n",
    "```python\n",
    "nlp = spacy.blank('en')\n",
    "optimizer = nlp.begin_training()\n",
    "for i in range(20):\n",
    "    random.shuffle(TRAIN_DATA)\n",
    "    for text, annotations in TRAIN_DATA:\n",
    "        nlp.update([text], [annotations], sgd=optimizer)\n",
    "nlp.to_disk(\"/model\")\n",
    "```\n",
    "\n",
    "## Entity recognizer update\n",
    "Teach the entity recognizer to recognize a new category. For example, you may want spacy to recognize instances of\n",
    "\"Iphone\" and apply the entity label `GADGET`.\n",
    "\n",
    "spaCy recognizes the following entities out of the box:\n",
    "\n",
    "Type | Description\n",
    "-----|------------\n",
    "PERSON | People, including fictional.\n",
    "NORP | Nationalities or religious or political groups.\n",
    "FAC | Buildings, airports, highways, bridges, etc.\n",
    "ORG | Companies, agencies, institutions, etc.\n",
    "GPE | Countries, cities, states.\n",
    "LOC | Non-GPE locations, mountain ranges, bodies of water.\n",
    "PRODUCT | Objects, vehicles, foods, etc. (Not services.)\n",
    "EVENT | Named hurricanes, battles, wars, sports events, etc.\n",
    "WORK_OF_ART | Titles of books, songs, etc.\n",
    "LAW | Named documents made into laws.\n",
    "LANGUAGE | Any named language.\n",
    "DATE | Absolute or relative dates or periods.\n",
    "TIME | Times smaller than a day.\n",
    "PERCENT | Percentage, including \"%\".\n",
    "MONEY | Monetary values, including unit.\n",
    "QUANTITY | Measurements, as of weight or distance.\n",
    "ORDINAL | \"first\", \"second\", etc.\n",
    "CARDINAL | Numerals that do not fall under another type\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a text classifier\n",
    "\n",
    "[From this kaggle kernel](https://www.kaggle.com/poonaml/text-classification-using-spacy):\n",
    ">SpaCy provides classification model with multiple, non-mutually exclusive labels. You can change the model architecture rather easily, but by default, the TextCategorizer class uses a convolutional neural network to assign position-sensitive vectors to each word in the document. The TextCategorizer uses its own CNN model, to avoid sharing weights with the other pipeline components. The document tensor is then summarized by concatenating max and mean pooling, and a multilayer perceptron is used to predict an output vector of length nr_class, before a logistic activation is applied elementwise. The value of each output neuron is the probability that some class is present.\n",
    "\n",
    "### TextCategorizer\n",
    "This is the component plugged into the pipeline that runs the classification. [Docs](https://spacy.io/api/textcategorizer). Usage:\n",
    "```python\n",
    "# If no other TextCategorizer exists:\n",
    "textcat = nlp.create_pipe(\n",
    "    \"textcat\",\n",
    "    config={\n",
    "        \"exclusive_classes\": True,\n",
    "        \"architecture\": \"simple_cnn\",\n",
    "    }\n",
    ")\n",
    "nlp.add_pipe(textcat, last=True)\n",
    "\n",
    "# add label to text classifier\n",
    "textcat.add_label(\"POSITIVE\")\n",
    "textcat.add_label(\"NEGATIVE\")\n",
    "```\n",
    "\n",
    "config options:\n",
    "- `exclusive classes`: Make categories mutually exclusive. Defaults to `False`.\n",
    "- ` architecture`: Model architecture to use, see [architectures](https://spacy.io/api/textcategorizer#architectures) for details. Defaults to \"ensemble\".\n",
    "\n",
    "### Training loop: `nlp.update`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.util import minibatch, compounding\n",
    "import numpy as np\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions from spacy documentation\n",
    "def load_data(train, limit=0, split=0.8):\n",
    "    train_data = train\n",
    "    np.random.shuffle(train_data)\n",
    "    train_data = train_data[-limit:]\n",
    "    texts, labels = zip(*train_data)\n",
    "    cats = [{'POSITIVE': bool(label)} for label in labels]\n",
    "    split = int(len(train_data) * split)\n",
    "    return (texts[:split], cats[:split]), (texts[split:], cats[split:])\n",
    "\n",
    "def evaluate(tokenizer, textcat, texts, cats):\n",
    "    docs = (tokenizer(text) for text in texts)\n",
    "    tp = 1e-8  # True positives\n",
    "    fp = 1e-8  # False positives\n",
    "    fn = 1e-8  # False negatives\n",
    "    tn = 1e-8  # True negatives\n",
    "    for i, doc in enumerate(textcat.pipe(docs)):\n",
    "        gold = cats[i]\n",
    "        for label, score in doc.cats.items():\n",
    "            if label not in gold:\n",
    "                continue\n",
    "            if score >= 0.5 and gold[label] >= 0.5:\n",
    "                tp += 1.\n",
    "            elif score >= 0.5 and gold[label] < 0.5:\n",
    "                fp += 1.\n",
    "            elif score < 0.5 and gold[label] < 0.5:\n",
    "                tn += 1\n",
    "            elif score < 0.5 and gold[label] >= 0.5:\n",
    "                fn += 1\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    f_score = 2 * (precision * recall) / (precision + recall)\n",
    "    return {'textcat_p': precision, 'textcat_r': recall, 'textcat_f': f_score}\n",
    "\n",
    "#(\"Number of texts to train from\", int)\n",
    "n_texts=30000\n",
    "#You can increase texts count if you have more computational power. Limits the number of samples to use.\n",
    "\n",
    "#(\"Number of training iterations\", int))\n",
    "n_iter=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = [\n",
    "    (\"I ordered this product specifically to try out the three different types of products. However, when I received the product, I opened it up only to find three of the Barn Burner salsas - obviously not what I had ordered.<br /><br />Buy at your own risk - who knows what you'll end up getting.\",\n",
    "  0),\n",
    "    ('Terrific - easy to use and great taste.  Only complaint is that the price is MUCH too high.',\n",
    "  0),\n",
    "    ('I have bought several of the Vitality canned dog food products and have found them all to be of good quality. The product looks more like a stew than a processed meat and it smells better. My Labrador is finicky and she appreciates this product better than  most.',\n",
    "  1),\n",
    "    (\"I ordered this product specifically to try out the three different types of products. However, when I received the product, I opened it up only to find three of the Barn Burner salsas - obviously not what I had ordered.<br /><br />Buy at your own risk - who knows what you'll end up getting.\",\n",
    "  0),\n",
    "    ('Terrific - easy to use and great taste.  Only complaint is that the price is MUCH too high.',\n",
    "  0),\n",
    "    ('I have bought several of the Vitality canned dog food products and have found them all to be of good quality. The product looks more like a stew than a processed meat and it smells better. My Labrador is finicky and she appreciates this product better than  most.',\n",
    "  1),\n",
    "    (\"I ordered this product specifically to try out the three different types of products. However, when I received the product, I opened it up only to find three of the Barn Burner salsas - obviously not what I had ordered.<br /><br />Buy at your own risk - who knows what you'll end up getting.\",\n",
    "  0),\n",
    "    ('Terrific - easy to use and great taste.  Only complaint is that the price is MUCH too high.',\n",
    "  0),\n",
    "    ('I have bought several of the Vitality canned dog food products and have found them all to be of good quality. The product looks more like a stew than a processed meat and it smells better. My Labrador is finicky and she appreciates this product better than  most.',\n",
    "  1),\n",
    "    (\"I ordered this product specifically to try out the three different types of products. However, when I received the product, I opened it up only to find three of the Barn Burner salsas - obviously not what I had ordered.<br /><br />Buy at your own risk - who knows what you'll end up getting.\",\n",
    "  0),\n",
    "    ('Terrific - easy to use and great taste.  Only complaint is that the price is MUCH too high.',\n",
    "  0),\n",
    "    ('I have bought several of the Vitality canned dog food products and have found them all to be of good quality. The product looks more like a stew than a processed meat and it smells better. My Labrador is finicky and she appreciates this product better than  most.',\n",
    "  1),\n",
    "    (\"I ordered this product specifically to try out the three different types of products. However, when I received the product, I opened it up only to find three of the Barn Burner salsas - obviously not what I had ordered.<br /><br />Buy at your own risk - who knows what you'll end up getting.\",\n",
    "  0),\n",
    "    ('Terrific - easy to use and great taste.  Only complaint is that the price is MUCH too high.',\n",
    "  0),\n",
    "    ('I have bought several of the Vitality canned dog food products and have found them all to be of good quality. The product looks more like a stew than a processed meat and it smells better. My Labrador is finicky and she appreciates this product better than  most.',\n",
    "  1),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading food reviews data...\n",
      "Using 15 examples (12 training, 3 evaluation)\n"
     ]
    }
   ],
   "source": [
    "# add the text classifier to the pipeline if it doesn't exist\n",
    "# nlp.create_pipe works for built-ins that are registered with spaCy\n",
    "if 'textcat' not in nlp.pipe_names:\n",
    "    textcat = nlp.create_pipe('textcat')\n",
    "    nlp.add_pipe(textcat, last=True)\n",
    "# otherwise, get it, so we can add labels to it\n",
    "else:\n",
    "    textcat = nlp.get_pipe('textcat')\n",
    "\n",
    "# add label to text classifier\n",
    "textcat.add_label('POSITIVE')\n",
    "\n",
    "# load the dataset\n",
    "print(\"Loading food reviews data...\")\n",
    "(train_texts, train_cats), (dev_texts, dev_cats) = load_data(train, limit=n_texts)\n",
    "print(\"Using {} examples ({} training, {} evaluation)\"\n",
    "      .format(len(train_texts) + len(dev_texts), len(train_texts), len(dev_texts)))\n",
    "train_data = list(zip(train_texts,\n",
    "                      [{'cats': cats} for cats in train_cats]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model...\n",
      "LOSS \t  P  \t  R  \t  F  \n",
      "0.048\t0.500\t0.000\t0.000\n",
      "0.022\t0.500\t0.000\t0.000\n",
      "0.018\t1.000\t1.000\t1.000\n",
      "0.015\t1.000\t1.000\t1.000\n",
      "0.013\t1.000\t1.000\t1.000\n",
      "0.004\t1.000\t1.000\t1.000\n",
      "0.004\t1.000\t1.000\t1.000\n",
      "0.000\t1.000\t1.000\t1.000\n",
      "0.001\t1.000\t1.000\t1.000\n",
      "0.001\t1.000\t1.000\t1.000\n"
     ]
    }
   ],
   "source": [
    "# get names of other pipes to disable them during training\n",
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'textcat']\n",
    "with nlp.disable_pipes(*other_pipes):  # only train textcat\n",
    "    optimizer = nlp.begin_training()\n",
    "    print(\"Training the model...\")\n",
    "    print('{:^5}\\t{:^5}\\t{:^5}\\t{:^5}'.format('LOSS', 'P', 'R', 'F'))\n",
    "    for i in range(n_iter):\n",
    "        losses = {}\n",
    "        # batch up the examples using spaCy's minibatch\n",
    "        batches = minibatch(train_data, size=compounding(4., 32., 1.001))\n",
    "        for batch in batches:\n",
    "            texts, annotations = zip(*batch)\n",
    "            nlp.update(texts, annotations, sgd=optimizer, drop=0.2,\n",
    "                       losses=losses)\n",
    "        with textcat.model.use_params(optimizer.averages):\n",
    "            # evaluate on the dev data split off in load_data()\n",
    "            scores = evaluate(nlp.tokenizer, textcat, dev_texts, dev_cats)\n",
    "        print('{0:.3f}\\t{1:.3f}\\t{2:.3f}\\t{3:.3f}'  # print a simple table\n",
    "              .format(losses['textcat'], scores['textcat_p'],\n",
    "                      scores['textcat_r'], scores['textcat_f']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('This tea is fun to watch as the flower expands in the water. Very smooth taste and can be used again and again in the same day. If you love tea, you gotta try these \"flowering teas\"',\n",
       " {'POSITIVE': 0.16774575412273407})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test the trained model\n",
    "test_text1 = 'This tea is fun to watch as the flower expands in the water. Very smooth taste and can be used again and again in the same day. If you love tea, you gotta try these \"flowering teas\"'\n",
    "test_text2=\"I bought this product at a local store, not from this seller.  I usually use Wellness canned food, but thought my cat was bored and wanted something new.  So I picked this up, knowing that Evo is a really good brand (like Wellness).<br /><br />It is one of the most disgusting smelling cat foods I've ever had the displeasure of using.  I was gagging while trying to put it into the bowl.  My cat took one taste and walked away, and chose to eat nothing until I replaced it 12 hours later with some dry food.  I would try another flavor of their food - since I know it's high quality - but I wouldn't buy the duck flavor again.\"\n",
    "doc = nlp(test_text2)\n",
    "test_text1, doc.cats"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
